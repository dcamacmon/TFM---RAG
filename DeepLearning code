#Document loading

! pip install langchain

import os
import openai
import fitz  # PyMuPDF
import sys
sys.path.append('../..')

#Loading the local environment
from dotenv import load_dotenv, find_dotenv
_ = load_dotenv(find_dotenv()) # read local .env file

openai.api_key  = os.environ['OPENAI_API_KEY']

import os


# Ruta de la carpeta con PDFs
directory = "D:/ESTUDIOS/UOC/Trimestre 2.2/TFM - RAG/PDF STORE/Store"

# Lista para almacenar el contenido extraído
docs_content = []

# Iterar sobre los archivos en la carpeta
for filename in os.listdir(directory):
    if filename.endswith(".pdf"):
        filepath = os.path.join(directory, filename)
        
        # Abrir el PDF y extraer el texto
        with fitz.open(filepath) as pdf:
            text = "\n".join([page.get_text("text") for page in pdf])
        
        docs_content.append(text)

# Unir el texto de todos los PDFs en una sola variable
txt = "\n".join(docs_content)

#Loading PDF's 
from langchain.document_loaders import PyPDFLoader
loader = PyPDFLoader("docs/cs229_lectures/MachineLearning-Lecture01.pdf")
pages = loader.load()


#Document splitting

from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter

    #Specific size of chunk and overlap
chunk_size =26
chunk_overlap = 4

      #Recursive splitting
r_splitter = RecursiveCharacterTextSplitter(
    chunk_size=chunk_size,
    chunk_overlap=chunk_overlap
)

      #Text splitting
c_splitter = CharacterTextSplitter(
    separator="\n",
    chunk_size=1000,
    chunk_overlap=150,
    length_function=len
)

      #Token splitting
from langchain.text_splitter import TokenTextSplitter
text_splitter = TokenTextSplitter(
    chunk_size=1, 
    chunk_overlap=0)

      #Markdown splitter
from langchain.document_loaders import NotionDirectoryLoader
from langchain.text_splitter import MarkdownHeaderTextSplitter

headers_to_split_on = [
    ("#", "Header 1"),
    ("##", "Header 2"),
    ("###", "Header 3"),
]

markdown_splitter = MarkdownHeaderTextSplitter(
    headers_to_split_on=headers_to_split_on
)
md_header_splits = markdown_splitter.split_text(markdown_document)



# Vectorstores and Embedding

from langchain.document_loaders import PyPDFLoader

# Load PDF
loaders = [
    # Duplicate documents on purpose - messy data
    PyPDFLoader("docs/cs229_lectures/MachineLearning-Lecture01.pdf"),
    PyPDFLoader("docs/cs229_lectures/MachineLearning-Lecture01.pdf"),
    PyPDFLoader("docs/cs229_lectures/MachineLearning-Lecture02.pdf"),
    PyPDFLoader("docs/cs229_lectures/MachineLearning-Lecture03.pdf")
]
docs = []
for loader in loaders:
    docs.extend(loader.load())

from langchain.embeddings.openai import OpenAIEmbeddings

#Pregunta que voy a realizar, k=nº docs a recuperar
  #Similarity_search es la función para búsqueda por similaridad
question= "WHAT I WANT TO ASK"
docs = vectordb.similarity_search(question,k=5)
  #MMR busca por similaridad, pero documentos diversos
docs_mmr = vectordb.max_marginal_relevance_search(question,k=3)


embedding = OpenAIEmbeddings()



# ! pip install chromadb
from langchain.vectorstores import Chroma
persist_directory = 'docs/chroma/'
!rm -rf ./docs/chroma  # remove old database files if any
vectordb = Chroma.from_documents(
    documents=splits,
    embedding=embedding,
    persist_directory=persist_directory
)


#Retrieval

from langchain.llms import OpenAI
from langchain.retrievers.self_query.base import SelfQueryRetriever
from langchain.chains.query_constructor.base import AttributeInfo

metadata_field_info = [
    AttributeInfo(
        name="source",
        description="The lecture the chunk is from, should be one of `docs/cs229_lectures/MachineLearning-Lecture01.pdf`, `docs/cs229_lectures/MachineLearning-Lecture02.pdf`, or `docs/cs229_lectures/MachineLearning-Lecture03.pdf`",
        type="string",
    ),
    AttributeInfo(
        name="page",
        description="The page from the lecture",
        type="integer",
    ),

#LLM GPT-3.5
document_content_description = "Lecture notes"
llm = OpenAI(model='gpt-3.5-turbo-instruct', temperature=0)
retriever = SelfQueryRetriever.from_llm(
    llm,
    vectordb,
    document_content_description,
    metadata_field_info,
    verbose=True
)


#Question Answering

from langchain.chains import RetrievalQA
qa_chain = RetrievalQA.from_chain_type(
    llm,
    retriever=vectordb.as_retriever()
)
result = qa_chain({"query": question})
result["result"]


#Chat



#Conclusion
