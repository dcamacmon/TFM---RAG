import os
import openai
import sys
sys.path.append('../..')

from dotenv import load_dotenv, find_dotenv
_ = load_dotenv(find_dotenv()) # read local .env file

openai.api_key  = os.environ['OPENAI_API_KEY']

from langchain.document_loaders import PyPDFLoader

# Load PDF
pdf_directory = "D:/ESTUDIOS/UOC/Trimestre 2.2/TFM - RAG/PDF STORE/Store"

# Crear la lista de loaders
loaders = [
    PyPDFLoader(os.path.join(pdf_directory, pdf_file))
    for pdf_file in os.listdir(pdf_directory) if pdf_file.endswith(".pdf")
]

# Cargar los documentos
docs = []
for loader in loaders:
    pdf_docs = loader.load()
    for doc in pdf_docs:
        # Agregar metadata con el nombre del archivo PDF y la página
        doc.metadata["source"] = os.path.basename(loader.file_path)  # Nombre del archivo
        doc.metadata["page"] = doc.metadata.get("page", 0)  # Página (si existe en metadata)
    docs.extend(pdf_docs)

# Split
from langchain.text_splitter import RecursiveCharacterTextSplitter
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size = 1500,
    chunk_overlap = 150
)
splits = text_splitter.split_documents(docs)

#Embedding
from langchain.embeddings.openai import OpenAIEmbeddings
embedding = OpenAIEmbeddings()


#Vectorstore
! pip install chromadb  # Asegúrate de tener ChromaDB instalado
from langchain.vectorstores import Chroma

persist_directory = 'docs/chroma/' #Directory to vectorstore
!rm -rf ./docs/chroma  #Delete the old directory (if it exists)

# Crear la base de datos de vectores con documentos divididos (splits)
vectordb = Chroma.from_documents(
    documents=splits,       
    embedding=embedding,    
    persist_directory=persist_directory
)

#Retrieval
from langchain.retrievers.self_query.base import SelfQueryRetriever
from langchain.chains.query_constructor.base import AttributeInfo

metadata_field_info = [
    AttributeInfo(
        name="source",
        description="The lecture the chunk is from (PDF source file).",
        type="string",
    ),
    AttributeInfo(
        name="page",
        description="The page number from the lecture notes.",
        type="integer",
    ),
]


document_content_description = "Lecture notes"
  
llm = OpenAI(model='gpt-3.5-turbo-instruct', temperature=0)
retriever = SelfQueryRetriever.from_llm(
    llm=llm,
    vectorstore=vectordb,
    document_content_description=document_content_description,
    metadata_field_info=metadata_field_info,
    verbose=True
)
docs = retriever.get_relevant_documents(question)
  
for d in docs:
    print(f"Source: {d.metadata['source']}, Page: {d.metadata['page']}")
    print(d.page_content[:500])  # Muestra los primeros 500 caracteres
    print("-" * 80)

#Generamos una respuesta en base a los documentos

from langchain.chains import RetrievalQA

template = """Use the following pieces of context to answer the question at the end. 
If you don't know the answer, just say that you don't know, don't try to make up an answer. 
Use three sentences maximum. Keep the answer as concise as possible. 
Always say "thanks for asking!" at the end of the answer. 

{context}
Question: {question}
Helpful Answer:"""

qa_chain = RetrievalQA.from_chain_type(
    llm,
    retriever=vectordb.as_retriever(),
    return_source_documents=True,
    chain_type_kwargs={"prompt": QA_CHAIN_PROMPT}
)
  
result = qa_chain({"query": question})
result["result"]
result["source_documents"][0]

##ALTERNATIVA MÁS CARA: MAP REDUCE
qa_chain_mr = RetrievalQA.from_chain_type(
    llm,
    retriever=vectordb.as_retriever(),
    chain_type="map_reduce"
)
result = qa_chain_mr({"query": question})
result["result"]

#Añadimos memoria al modelo
from langchain.memory import ConversationBufferMemory
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)

from langchain.chains import ConversationalRetrievalChain
retriever=vectordb.as_retriever()
qa = ConversationalRetrievalChain.from_llm(
    llm,
    retriever=retriever,
    memory=memory
)

result = qa({"question": question})

